{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef02a87-b556-4739-9528-49243584ff2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "#\n",
    "# This file is part of b-swarm\n",
    "# Licensed under the GNU General Public License version 3 (GPLv3)\n",
    "#\n",
    "# Author: BB, 2024\n",
    "# __version__ = \"code_version\"\n",
    "__license__ = \"gplv3\"\n",
    "__author__ = \"bb\"\n",
    "__version__ = \"0.3\"\n",
    "\n",
    "\n",
    "from binascii import unhexlify\n",
    "from io import BytesIO\n",
    "from math import sqrt, prod, log2\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ppdeep import compare as ppcompare\n",
    "from PIL import Image, ImageChops\n",
    "\n",
    "\n",
    "def ppdeep_diff(ppdeep1, ppdeep2):\n",
    "    if ppdeep1 and ppdeep2:\n",
    "        return ppcompare(ppdeep1, ppdeep2)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def sha256_match(hash1, hash2):\n",
    "    if hash1 and hash2:\n",
    "        return hash1 == hash2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def ssl_match(fprint1, fprint2):\n",
    "    if fprint1 and fprint2:\n",
    "        return fprint1 == fprint2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def url_match(url1, url2):\n",
    "    if url1 and url2:\n",
    "        if url1.endswith('/'):\n",
    "            url1 = url1[:-1]\n",
    "        if url2.endswith('/'):\n",
    "            url2 = url2[:-1]\n",
    "        return url1 == url2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def image_load(image, grayscale=True, grayscaleconvert=False, size=(1500, 3000), resize=False, subsample=False, factor=2):\n",
    "    if image:\n",
    "        image = unhexlify(image)\n",
    "        pilImage = Image.open(BytesIO(image))\n",
    "        if resize:\n",
    "            resizedImage = pilImage.resize(size)\n",
    "        if grayscaleconvert:\n",
    "            grayscaleImage = pilImage.convert(\"L\")\n",
    "    else:\n",
    "        if grayscale:\n",
    "            pilImage = Image.new('L', (size[0], size[1]))\n",
    "        else:\n",
    "            pilImage = Image.new('RGB', (size[0], size[1]), (0xff, 0xff, 0xff))\n",
    "    if subsample:\n",
    "        imageArray = np.array(pilImage)\n",
    "        pilImage = Image.fromarray(imageArray[::factor, ::factor])\n",
    "    return pilImage\n",
    "\n",
    "def image_mse(image1, image2):\n",
    "    npImage1 = np.array(image1)\n",
    "    npImage2 = np.array(image2)\n",
    "    squared_diff = np.square(npImage1 - npImage2)\n",
    "    mse = np.mean(squared_diff)\n",
    "    return mse\n",
    "\n",
    "def image_diff(image1, image2):\n",
    "    diff = ImageChops.difference(image1, image2)\n",
    "    return diff\n",
    "\n",
    "def calculate_rmse(mseValues):\n",
    "    sqMseSum = sum(mse ** 2 for mse in mseValues)\n",
    "    rmse = sqrt(sqMseSum / len(mseValues))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40eb86b0-89b6-4f6d-9395-27ba01421b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import clickhouse_connect as cc\n",
    "\n",
    "\n",
    "def db_connect(db_host=\"localhost\", db_username=\"default\", db_password=\"\"):\n",
    "    client = cc.get_client(host=db_host, username=db_username, password=db_password)\n",
    "    return client\n",
    "\n",
    "def load_db(cc_client, snapshot_db=\"harvester\", snapshot_table=\"snapshot\", diff_table=\"cluster\", snapshot_file=\"report*.parquet\", diff_file=\"*_cluster.parquet\", load_diff=False, clean_init=True):\n",
    "    \"\"\"\n",
    "    Initialize the Clickhouse database.\n",
    "    Load the Parquet files in a database.\n",
    "    \"\"\"\n",
    "    if clean_init:\n",
    "        cc_client.command(f\"DROP TABLE IF EXISTS {snapshot_db}.{snapshot_table}\")\n",
    "        cc_client.command(f\"DROP TABLE IF EXISTS {snapshot_db}.{diff_table}\")\n",
    "        cc_client.command(f\"DROP DATABASE IF EXISTS {snapshot_db}\")\n",
    "        cc_client.command(f\"CREATE DATABASE {snapshot_db}\")\n",
    "    try:\n",
    "        cc_client.command(f\"CREATE TABLE {snapshot_db}.{snapshot_table} ENGINE = MergeTree ORDER BY tuple() AS SELECT * FROM file('{snapshot_file}', Parquet)\")\n",
    "        if load_diff:\n",
    "            cc_client.command(f\"CREATE TABLE {snapshot_db}.{diff_table} ENGINE = MergeTree ORDER BY tuple() AS SELECT * FROM file('{diff_file}', Parquet)\")\n",
    "    except Exception as err:\n",
    "        return err\n",
    "\n",
    "def insert_cluster_into_table(cc_client, dataframe=None, snapshot_db=\"harvester\", diff_table=\"cluster\", force_schema=True, init_table=False):\n",
    "    if init_table:\n",
    "        cc_client.command(f\"DROP TABLE IF EXISTS {snapshot_db}.{diff_table}\")\n",
    "        cc_client.command(f\"\"\"CREATE TABLE IF NOT EXISTS {snapshot_db}.{diff_table} (\n",
    "                              meta_taskid String,\n",
    "                              http_url String,\n",
    "                              http_document_embedding_type String,\n",
    "                              http_document_embeddings Array(Float64),\n",
    "                              http_image_embedding_type String,\n",
    "                              http_image_embeddings Array(Float64),\n",
    "                              cluster_type String,\n",
    "                              cluster_data Array(UInt64)\n",
    "                          ) ENGINE = MergeTree()\n",
    "                          ORDER BY meta_taskid;\n",
    "                          \"\"\")\n",
    "    if not dataframe and init_table:\n",
    "        return \"Init\"\n",
    "    if not dataframe:\n",
    "        return False\n",
    "    if force_schema:\n",
    "        schema = {\n",
    "            \"meta_taskid\": \"string\",\n",
    "            \"http_url\": \"string\",\n",
    "            \"http_document_embedding_type\": \"string\",\n",
    "            \"http_document_embeddings\": \"object\",\n",
    "            \"http_image_embedding_type\": \"string\",\n",
    "            \"http_image_embeddings\": \"object\",\n",
    "            \"cluster_type\": \"string\",\n",
    "            \"cluster_data\": \"object\"\n",
    "        }\n",
    "        dataframe = dataframe.astype(schema)\n",
    "    status = cc_client.insert_df(database=snapshot_db, table=diff_table, df=dataframe)\n",
    "    return status\n",
    "\n",
    "def export_cluster_to_parquet(cc_client, diff_file=None, snapshot_db=\"harvester\", diff_table=\"cluster\", method=\"clickhouse\"):\n",
    "    \"\"\"\n",
    "    Export the clustering Table to Parquet file\n",
    "    \"\"\"\n",
    "    if not diff_file:\n",
    "        agentId = cc_client.command(f\"SELECT meta_taskid from {snapshot_db}.{diff_table} LIMIT 1\")\n",
    "        taskId = agentId.split(\":\")[0]\n",
    "        diff_file = f\"{taskId}_cluster.parquet\"\n",
    "    if method.lower() == \"clickhouse\":\n",
    "        try:\n",
    "            status = cc_client.command(f\"SET engine_file_allow_create_multiple_files = 1\")\n",
    "            status = cc_client.command(f\"INSERT INTO FUNCTION file('{diff_file}') SELECT * FROM {snapshot_db}.{diff_table} FORMAT Parquet SETTINGS compression = 'snappy'\")\n",
    "            return status\n",
    "        except Exception as err:\n",
    "            return err\n",
    "    if method.lower() == \"pandas\":\n",
    "        df = cc_client.query_df(f\"SELECT * FROM {snapshot_db}.{diff_table} ORDER BY meta_taskid ASC\")\n",
    "        try:\n",
    "            df.to_parquet(diff_file, engine=\"fastparquet\", index=False, compression=\"snappy\")\n",
    "        except Exception as err:\n",
    "            return err\n",
    "\n",
    "\n",
    "client = db_connect()\n",
    "load_db(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fa8276f-5e36-43e5-82f8-f29d6e7a8ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Init'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insert_cluster_into_table(client, init_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "430df862-dd43-4af9-a53f-8664ab931f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://google.com\n",
      "Extracting HTML document embeddings...\n",
      "Extracting PNG image embeddings...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Done\n",
      "CPU times: user 7.39 s, sys: 537 ms, total: 7.93 s\n",
      "Wall time: 7.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from html_sanitizer import Sanitizer\n",
    "from readability import Document\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mnv2_preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as rn50v2_preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as kimage\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from ipywidgets import HBox\n",
    "from ipywidgets import Image as ipyImage\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def get_html_content(htmlDoc, sanitize=True, content=\"content\", postprocess=True, markdown=False):\n",
    "    noDoc = \"NONE\" # FIXME: No embedding extraction possible on empty set\n",
    "    if not htmlDoc:\n",
    "        return noDoc\n",
    "    if sanitize:\n",
    "        sanitizer = Sanitizer()\n",
    "        htmlDoc = sanitizer.sanitize(htmlDoc)\n",
    "        if not htmlDoc:\n",
    "            return noDoc\n",
    "    htmlDoc = Document(htmlDoc)\n",
    "    if content.lower() == \"summary\":\n",
    "        htmlContent = htmlDoc.summary()\n",
    "    else:\n",
    "        htmlContent = htmlDoc.content()\n",
    "    if postprocess:\n",
    "        soup = bs(htmlContent, 'html.parser')\n",
    "        htmlContent = soup.get_text(separator=' ', strip=True)\n",
    "    if markdown:\n",
    "        htmlContent = md(htmlContent)\n",
    "    return str(htmlContent)\n",
    "\n",
    "\n",
    "def display_image_cluster(image_list, cluster_list, image_ratio=5):\n",
    "    imgWidgets = []\n",
    "    for image in image_list:\n",
    "        img = image_load(image)\n",
    "        IMAGE_RATIO = image_ratio\n",
    "        reimg = ( img.resize((\n",
    "            int(img.width // IMAGE_RATIO),\n",
    "            int(img.height // IMAGE_RATIO)\n",
    "            )) )\n",
    "        imgWidgets.append(ipyImage(value=reimg._repr_png_(), format='png'))\n",
    "    clusters = {}\n",
    "    for i in range(0, len(cluster_list)):\n",
    "        cluster = cluster_list[i]\n",
    "        widget = imgWidgets[i]\n",
    "        if cluster not in clusters.keys():\n",
    "            clusters[cluster] = []\n",
    "        clusters[cluster].append(widget)\n",
    "    imgHboxes = []\n",
    "    for key, val in clusters.items():\n",
    "        print(f\"[+] Cluster: {key} length {len(val)}\")\n",
    "        display(HBox(val))\n",
    "\n",
    "\n",
    "\n",
    "snapshotDb = \"harvester\"\n",
    "snapshotTable = \"snapshot\"\n",
    "urlList = client.query(f\"SELECT DISTINCT http_url FROM {snapshotDb}.{snapshotTable} ORDER BY http_url ASC\").result_columns[0]\n",
    "agentList = client.query(f\"SELECT DISTINCT meta_agentid FROM {snapshotDb}.{snapshotTable} ORDER BY meta_agentid ASC\").result_columns[0]\n",
    "\n",
    "url = urlList[12]\n",
    "print(url)\n",
    "frame = client.query_df(f\"SELECT * FROM {snapshotDb}.{snapshotTable} WHERE http_url=='{url}' ORDER BY meta_agentid ASC\")\n",
    "frame['http_document_content'] = frame['http_document'].apply(get_html_content, postprocess=False)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "### HTML embeddings TFIDF\n",
    "print(\"Extracting HTML document embeddings...\")\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "html_embeddings = vectorizer.fit_transform(frame['http_document_content'])\n",
    "html_embeddings = html_embeddings.toarray()\n",
    "html_embeddings = html_embeddings.tolist()\n",
    "\n",
    "print(\"Extracting PNG image embeddings...\")\n",
    "\n",
    "### PNG image embeddings - VGG16\n",
    "vgg16Model = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False, pooling='avg')\n",
    "\n",
    "def get_image_embedding_vgg16(binImage, IMAGE_SIZE = 224):\n",
    "    if binImage == None:\n",
    "        return np.zeros((vgg16Model.output_shape[1],))\n",
    "    try:\n",
    "        binImage = unhexlify(binImage)\n",
    "        img = Image.open(BytesIO(binImage))\n",
    "        if img.mode == 'L':\n",
    "            img = Image.merge(\"RGB\", (img, img, img))\n",
    "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        img_array = np.array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = vgg16_preprocess_input(img_array)\n",
    "        assert imgArray.shape == (1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        assert imgArray.dtype == np.float32\n",
    "        features = vggModel.predict(img_array)\n",
    "        return features.flatten()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return np.zeros((vgg16Model.output_shape[1],))\n",
    "\n",
    "### PNG image embeddings - ResNet50V2\n",
    "base_model = ResNet50V2(weights='imagenet', input_shape=(224, 224, 3))\n",
    "rn50v2Model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
    "\n",
    "def get_image_embedding_rn50v2(binImage, IMAGE_SIZE = 224, RN_DIMENSIONS = 2048):\n",
    "    if binImage == None:\n",
    "        return np.zeros((RN_DIMENSIONS,))\n",
    "    try:\n",
    "        binImage = unhexlify(binImage)\n",
    "        img = Image.open(BytesIO(binImage))\n",
    "        if img.mode == 'L':\n",
    "            img = Image.merge(\"RGB\", (img, img, img))\n",
    "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        imgArray = np.array(img)\n",
    "        imgArray = np.expand_dims(imgArray, axis=0)\n",
    "        imgArray = rn50v2_preprocess_input(imgArray)\n",
    "        assert imgArray.shape == (1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        assert imgArray.dtype == np.float32\n",
    "        features = rn50v2Model.predict(imgArray)\n",
    "        return features.flatten()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return np.zeros((RN_DIMENSIONS,))\n",
    "\n",
    "### PNG image embeddings - MobileNetV2\n",
    "base_model = MobileNetV2(weights='imagenet', input_shape=(224, 224, 3), include_top=False, pooling='avg')\n",
    "mnv2Model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def get_image_embedding_mnv2(binImage, IMAGE_SIZE = 224, MN_DIMENSIONS = 1280):\n",
    "    if binImage == None:\n",
    "        return np.zeros((MN_DIMENSIONS,))\n",
    "    try:\n",
    "        binImage = unhexlify(binImage)\n",
    "        img = Image.open(BytesIO(binImage))\n",
    "        if img.mode == 'L':\n",
    "            img = Image.merge(\"RGB\", (img, img, img))\n",
    "        img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        imgArray = np.array(img)\n",
    "        imgArray = np.expand_dims(imgArray, axis=0)\n",
    "        imgArray = mnv2_preprocess_input(imgArray)\n",
    "        assert imgArray.shape == (1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "        assert imgArray.dtype == np.float32\n",
    "        features = mnv2Model.predict(imgArray)\n",
    "        return features.flatten()\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return np.zeros((MN_DIMENSIONS,))\n",
    "\n",
    "image_embeddings = [get_image_embedding_rn50v2(image) for image in frame['http_image']]\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "63eb1ccd-8173-4873-8052-d75300693c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  0  0 -1  0  0  1  0 -1  0  0  0  0  0  1  0 -1]\n",
      "[+] Cluster: 0 length 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6750b2162e48ad82c4a14ab66adb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01,\\x00\\x00\\x02X\\x08\\x00\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Cluster: 1 length 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f59bb8ff6d489bb45fb290cd1795b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01,\\x00\\x00\\x02X\\x08\\x00\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Cluster: -1 length 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fb6d077425472ca03d8c4b9888885f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01,\\x00\\x00\\x02X\\x08\\x00\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 704 ms, sys: 9.65 ms, total: 714 ms\n",
      "Wall time: 767 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.cluster import HDBSCAN, OPTICS\n",
    "\n",
    "\n",
    "#combined_embeddings = [np.concatenate((html_emb, img_emb)) for html_emb, img_emb in zip(html_embeddings, image_embeddings)]\n",
    "combined_embeddings = np.hstack((html_embeddings, image_embeddings))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaledEmbeddings = scaler.fit_transform(combined_embeddings)\n",
    "algorithm = HDBSCAN(min_cluster_size=3, allow_single_cluster=False,\n",
    "                    cluster_selection_method=\"eom\", leaf_size=20,\n",
    "                    n_jobs=2)\n",
    "#algorithm = OPTICS(min_samples=3, leaf_size=10, n_jobs=2)\n",
    "labels = algorithm.fit_predict(scaledEmbeddings)\n",
    "print(labels)\n",
    "\n",
    "images = client.query(f\"select http_image from harvester.snapshot where http_url=='{url}' order by meta_agentid asc\").result_columns[0]\n",
    "display_image_cluster(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0222c8b6-2ce8-49cb-8f1f-29b7cce408d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       http.entropy  http_content_bytesize  http_content_tags  \\\n",
      "count     18.000000              18.000000          18.000000   \n",
      "mean       5.371936          291614.555556         136.222222   \n",
      "std        0.003512           10200.699747           6.847546   \n",
      "min        5.367097          278051.000000         127.000000   \n",
      "25%        5.368451          280318.750000         130.000000   \n",
      "50%        5.371776          297984.000000         140.000000   \n",
      "75%        5.374728          300451.000000         141.000000   \n",
      "max        5.377534          302640.000000         145.000000   \n",
      "\n",
      "       http_content_uniquetags  http_content_tagdepth  http_content_attributes  \n",
      "count                     18.0              18.000000                18.000000  \n",
      "mean                      10.0               3.036793               446.055556  \n",
      "std                        0.0               0.001865                25.521553  \n",
      "min                       10.0               3.034483               412.000000  \n",
      "25%                       10.0               3.035461               423.000000  \n",
      "50%                       10.0               3.035716               460.000000  \n",
      "75%                       10.0               3.038462               464.000000  \n",
      "max                       10.0               3.039370               477.000000  \n",
      "       http_image_structentropy  http_image_dataentropy  http_image_edges  \\\n",
      "count                      18.0               18.000000              18.0   \n",
      "mean                        0.0                2.561965               0.0   \n",
      "std                         0.0                0.000000               0.0   \n",
      "min                         0.0                2.561965               0.0   \n",
      "25%                         0.0                2.561965               0.0   \n",
      "50%                         0.0                2.561965               0.0   \n",
      "75%                         0.0                2.561965               0.0   \n",
      "max                         0.0                2.561965               0.0   \n",
      "\n",
      "       http_image_lbp  \n",
      "count    1.800000e+01  \n",
      "mean     1.797023e-02  \n",
      "std      3.570031e-18  \n",
      "min      1.797023e-02  \n",
      "25%      1.797023e-02  \n",
      "50%      1.797023e-02  \n",
      "75%      1.797023e-02  \n",
      "max      1.797023e-02  \n",
      "[0, 2, 1, 0, 1, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0] \n",
      " [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[+] Cluster: -1 length 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a839742fc347efb73ab3bc6c5ab829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x01,\\x00\\x00\\x02X\\x08\\x00\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 720 ms, sys: 18.4 ms, total: 739 ms\n",
      "Wall time: 858 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN, OPTICS\n",
    "\n",
    "\n",
    "def get_dbscan_clusters(dataFrame, eps=0.2, samples=3):\n",
    "    scaler = RobustScaler()\n",
    "    scaledFrame = scaler.fit_transform(dataFrame)\n",
    "    engine = DBSCAN(eps=eps, min_samples=samples)\n",
    "    return list(engine.fit(scaledFrame).labels_)\n",
    "\n",
    "def get_hdbscan_clusters(dataFrame, samples=3):\n",
    "    scaler = RobustScaler()\n",
    "    scaledFrame = scaler.fit_transform(dataFrame)\n",
    "    engine = HDBSCAN(min_cluster_size=samples)\n",
    "    return list(engine.fit(scaledFrame).labels_)\n",
    "\n",
    "def get_optics_clusters(dataFrame, eps=0.2, samples=3):\n",
    "    scaler = RobustScaler()\n",
    "    scaledFrame = scaler.fit_transform(dataFrame)\n",
    "    engine = OPTICS(max_eps=eps*2, min_samples=samples)\n",
    "    return list(engine.fit(scaledFrame).labels_)\n",
    "\n",
    "def get_clusters(dataFrame, scaler=\"robust\", algorithm=\"hdbscan\", samples=3, eps=0.5, maxeps=None):\n",
    "    if scaler.lower() == \"none\":\n",
    "        scaler = None\n",
    "    elif scaler.lower() == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler.lower() == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler.lower() == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    if algorithm.lower() == \"hdbscan\":\n",
    "        algorithm = HDBSCAN(min_cluster_size=samples)\n",
    "    elif algorithm.lower() == \"dbscan\":\n",
    "        algorithm = DBSCAN(eps=eps, min_samples=samples)\n",
    "    elif algorithm.lower() == \"optics\":\n",
    "        if not maxeps:\n",
    "            maxeps = eps * 2\n",
    "        algorithm = OPTICS(max_eps=maxeps, min_samples=samples)\n",
    "    else:\n",
    "        algorithm = None\n",
    "\n",
    "    if scaler:\n",
    "        dataFrame = scaler.fit_transform(dataFrame)\n",
    "    if algorithm:\n",
    "        clustering = algorithm.fit(dataFrame)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return list(clustering.labels_)\n",
    "\n",
    "print(contentFrame.describe())\n",
    "print(imageFrame.describe())\n",
    "\n",
    "txtlabels = get_clusters(contentFrame, scaler=\"robust\", algorithm=\"hdbscan\")\n",
    "imglabels = get_clusters(imageFrame, scaler=\"robust\", algorithm=\"hdbscan\")\n",
    "\n",
    "print(txtlabels, '\\n', imglabels)\n",
    "\n",
    "labels = imglabels\n",
    "\n",
    "images = client.query(f\"select http.image from harvester.snapshot where url.init=='{url}' order by meta.agentid asc\").result_columns[0]\n",
    "display_image_cluster(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91e731-1ed9-4357-8c3a-73e16a4bcc3b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "selectList = []\n",
    "for cluster in clusterList:\n",
    "    dbscanVal = sum(cluster[0][0])\n",
    "    fuzzyVal = cluster[0][1][1]\n",
    "    mseVal = cluster[0][1][3]\n",
    "    #print(dbscanVal, fuzzyVal, mseVal, dbscanVal == 0 or (fuzzyVal == 100.0 or mseVal == 0.0))\n",
    "    if dbscanVal == 0 or (fuzzyVal >= 100.0 - threshold * 10 or mseVal <= 0.0 + threshold):\n",
    "        continue\n",
    "    selectList.append((cluster[1], fuzzyVal, mseVal, cluster[0][0]))\n",
    "    print(cluster[1], fuzzyVal, mseVal, cluster[0][0])\n",
    "\n",
    "print(len(selectList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2c174-7ebb-4428-a055-41dc9e39796c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def load_url(button_click):\n",
    "    global snapshotDf\n",
    "    snapshotDf = client.query_df(f\"select * from {snapshotDb}.{snapshotTable} where url.init=='{urlLayer.value}' ORDER BY meta.agentid ASC\")\n",
    "    with output1:\n",
    "        current = {}\n",
    "        current['new'] = agentLayer1.value\n",
    "        update_output1(current)\n",
    "    with output2:\n",
    "        current = {}\n",
    "        current['new'] = agentLayer2.value\n",
    "        update_output2(current)\n",
    "    with output3:\n",
    "        output3.clear_output()\n",
    "        metadata3.value = \"\"\n",
    "    dbscanVal = str(clusterList[urllist.index(urlLayer.value)][0][0])\n",
    "    frameDiff = str(clusterList[urllist.index(urlLayer.value)][0][1])\n",
    "    hdbscanVal = str(clusterList[urllist.index(urlLayer.value)][0][2])\n",
    "    opticsVal = str(clusterList[urllist.index(urlLayer.value)][0][3])\n",
    "    metadata4.value = f\"{dbscanVal}\\n{hdbscanVal}\\n{opticsVal}\\n{frameDiff}\"\n",
    "        \n",
    "def single_snapshot():\n",
    "    return 0\n",
    "    analyze_snapshot(urllist, snapshotTable, singleUrl=urllist.index(urlLayer.value))\n",
    "\n",
    "def compare_images(button_click):\n",
    "    snapshotEntry1 = snapshotDf[(snapshotDf[\"meta.agentid\"] == agentList[agentLayer1.value])]\n",
    "    image1 = snapshotEntry1[\"http.image\"].item()\n",
    "    snapshotEntry2 = snapshotDf[(snapshotDf[\"meta.agentid\"] == agentList[agentLayer2.value])]\n",
    "    image2 = snapshotEntry2[\"http.image\"].item()\n",
    "    imgDiff = image_diff(image1, image2)\n",
    "    with output3:\n",
    "        output3.clear_output()\n",
    "        display(imgDiff[0].resize((\n",
    "            int(imgDiff[0].width // IMAGE_RATIO),\n",
    "            int(imgDiff[0].height // IMAGE_RATIO)\n",
    "            )))\n",
    "        ppdeep1 = snapshotEntry1['http.fuzzyhash'].item()\n",
    "        ppdeep2 = snapshotEntry2['http.fuzzyhash'].item()\n",
    "        sha256_1 = snapshotEntry1['http.sha256'].item()\n",
    "        sha256_2 = snapshotEntry2['http.sha256'].item()\n",
    "        ent1 = snapshotEntry1['http.entropy'].item()\n",
    "        ent2 = snapshotEntry2['http.entropy'].item()\n",
    "        metadata3.value = (f\"Content SHA256 match: {sha256_match(sha256_1, sha256_2)}\\n\"\n",
    "                           f\"Content Ppdeep similarity: {ppdeep_diff(ppdeep1, ppdeep2)}\\n\"\n",
    "                           f\"Image Mean squared error: {round(imgDiff[1], 4)}\\n\"\n",
    "                           f\"URL match: {url_match(snapshotEntry1['url.init'].item(), snapshotEntry1['url.end'].item())}\"\n",
    "                           f\" | {url_match(snapshotEntry2['url.init'].item(), snapshotEntry2['url.end'].item())}\\n\"\n",
    "                           f\"SSL match: {snapshotEntry1['http.ssl'].item() == snapshotEntry2['http.ssl'].item()}\"\n",
    "                          )\n",
    "\n",
    "def update_output1(change):\n",
    "    index = change['new']\n",
    "    with output1:\n",
    "        output1.clear_output()\n",
    "        snapshotEntry = snapshotDf[(snapshotDf[\"meta.agentid\"] == agentList[agentLayer1.value])]\n",
    "        image = image_load(snapshotEntry[\"http.image\"].item())\n",
    "        display(image.resize((\n",
    "            int(image.width // IMAGE_RATIO),\n",
    "            int(image.height // IMAGE_RATIO)\n",
    "            )))\n",
    "        metadata1.value = (f\"AgentID: {snapshotEntry['meta.agentid'].item()}\\n\"\n",
    "                           f\"URL: {snapshotEntry['url.init'].item()}\\n\"\n",
    "                           f\"Visited URL: {snapshotEntry['url.end'].item()}\\n\"\n",
    "                           f\"Collection time: {snapshotEntry['meta.interacttime'].item().microsecond/1000}s\\n\"\n",
    "                           f\"UserAgent: {snapshotEntry['http.useragent'].item()}\\n\"\n",
    "                           f\"SSL fingerprint: {snapshotEntry['http.ssl'].item()}\\n\"\n",
    "                           f\"SHA256: {snapshotEntry['http.sha256'].item()}\\n\"\n",
    "                           f\"Ppdeep: {snapshotEntry['http.fuzzyhash'].item()}\"\n",
    "                          )\n",
    "        \n",
    "def update_output2(change):\n",
    "    index = change['new']\n",
    "    with output2:\n",
    "        output2.clear_output()\n",
    "        snapshotEntry = snapshotDf[(snapshotDf[\"meta.agentid\"] == agentList[agentLayer2.value])]\n",
    "        image = image_load(snapshotEntry[\"http.image\"].item())\n",
    "        display(image.resize((\n",
    "            int(image.width // IMAGE_RATIO),\n",
    "            int(image.height // IMAGE_RATIO)\n",
    "            )))\n",
    "        metadata2.value = (f\"AgentID: {snapshotEntry['meta.agentid'].item()}\\n\"\n",
    "                           f\"URL: {snapshotEntry['url.init'].item()}\\n\"\n",
    "                           f\"Visited URL: {snapshotEntry['url.end'].item()}\\n\"\n",
    "                           f\"Collection time: {snapshotEntry['meta.interacttime'].item().microsecond/1000}s\\n\"\n",
    "                           f\"UserAgent: {snapshotEntry['http.useragent'].item()}\\n\"\n",
    "                           f\"SSL fingerprint: {snapshotEntry['http.ssl'].item()}\\n\"\n",
    "                           f\"SHA256: {snapshotEntry['http.sha256'].item()}\\n\"\n",
    "                           f\"Ppdeep: {snapshotEntry['http.fuzzyhash'].item()}\"\n",
    "                          )\n",
    "            \n",
    "def reload_images(button_click):\n",
    "    with output1:\n",
    "        current = {}\n",
    "        current['new'] = agentLayer1.value\n",
    "        update_output1(current)\n",
    "    with output2:\n",
    "        current = {}\n",
    "        current['new'] = agentLayer2.value\n",
    "        update_output2(current)\n",
    "    with output3:\n",
    "        output3.clear_output()\n",
    "        metadata3.value = \"\"\n",
    "\n",
    "\n",
    "IMAGE_RATIO = 3.5\n",
    "vboxWidth = \"455px\"\n",
    "\n",
    "urlLayer = widgets.Dropdown(options=urlList, value=urlList[0], description=\"url\", disabled=False)\n",
    "agentLayer1 = widgets.IntSlider(min=0, max=len(agentList) - 1, value=0)\n",
    "agentLayer2 = widgets.IntSlider(min=0, max=len(agentList) - 1, value=0)\n",
    "snapshotButton = widgets.Button(description=\"Analyze snapshot\")\n",
    "reloadButton = widgets.Button(description=\"Reload images\")\n",
    "compareButton = widgets.Button(description=\"Compare images\")\n",
    "\n",
    "urlLayer.observe(load_url, names='value')\n",
    "agentLayer1.observe(update_output1, names='value')\n",
    "agentLayer2.observe(update_output2, names='value')\n",
    "snapshotButton.on_click(single_snapshot)\n",
    "reloadButton.on_click(reload_images)\n",
    "compareButton.on_click(compare_images)\n",
    "\n",
    "textboxSize = widgets.Layout(flex='0 1 auto', width='450px')\n",
    "output1 = widgets.Output()\n",
    "metadata1 = widgets.Textarea(placeholder='Vantage1 metadata', layout=textboxSize)\n",
    "output2 = widgets.Output()\n",
    "metadata2 = widgets.Textarea(placeholder='Vantage2 metadata', layout=textboxSize)\n",
    "output3 = widgets.Output()\n",
    "metadata3 = widgets.Textarea(placeholder='Comparison results', layout=textboxSize)\n",
    "metadata4 = widgets.Textarea(placeholder='Snapshot clusters', layout=widgets.Layout(flex='0 1 auto', width='600px'))\n",
    "\n",
    "hbox1 = widgets.HBox([urlLayer, metadata4])\n",
    "vbox1 = widgets.VBox([agentLayer1, output1, metadata1], layout=widgets.Layout(width=vboxWidth))\n",
    "vbox2 = widgets.VBox([agentLayer2, output2, metadata2], layout=widgets.Layout(width=vboxWidth))\n",
    "hbox2 = widgets.HBox([compareButton])\n",
    "vbox3 = widgets.VBox([hbox2, output3, metadata3], layout=widgets.Layout(width=vboxWidth))\n",
    "hbox3 = widgets.HBox([vbox1, vbox2, vbox3])\n",
    "layout = widgets.VBox([hbox1, hbox3])\n",
    "display(layout)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_url(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c377d4-8106-40e8-ac6e-a9d11f7c63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLDCODE\n",
    "\n",
    "#def calculate_entropy(data): # replace by sklearn or scipy builtins\n",
    "#    \"\"\"\n",
    "#    Calculate the Shannon entorpy for the provided content.\n",
    "#    \"\"\"\n",
    "#    counter = Counter(data)\n",
    "#    length = float(len(data))\n",
    "#    ent = -sum(\n",
    "#        count / length * log2(count / length)\n",
    "#        for count in counter.values()\n",
    "#        )\n",
    "#    return ent\n",
    "\n",
    "#def calculate_arithmetic_mean(values, handlenone=True, nonevalue=0): # to be obsolete\n",
    "#    if handlenone:\n",
    "#        values = [nonevalue if value == None else value for value in values]\n",
    "#    return sum(values) / len(values)\n",
    "\n",
    "#def calculate_normalized_diff(ent1, ent2, handleinf=True, inf=1e32): # to be obsolete\n",
    "#    absDiff = abs(ent1 - ent2)\n",
    "#    average = (ent1 + ent2) / 2\n",
    "#    if average != 0:\n",
    "#        normDiff = absDiff / average\n",
    "#    else:\n",
    "#        if handleinf:\n",
    "#            normDiff = inf\n",
    "#        else:\n",
    "#            normDiff = float(\"inf\")\n",
    "#    return float(normDiff)\n",
    "\n",
    "#def convert_image_array(image, resize_width=128, resize=True):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    img = image_load(image)\n",
    "#    if resize:\n",
    "#        imgWidth, imgHeight = img.size\n",
    "#        aspect = imgHeight // imgWidth\n",
    "#        img = img.resize((resize_width, resize_width * aspect)).convert('L')\n",
    "#    return np.asarray(img).flatten()\n",
    "\n",
    "#def convert_sha256_value(sha256_hash):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    if sha256_hash:\n",
    "#        return int(sha256_hash, 16)\n",
    "#    else:\n",
    "#        return 0\n",
    "\n",
    "#def calculate_mean_ppdeep(ppdeepNpArray):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    hashDiffList = []\n",
    "#    npArrayLen = len(ppdeepNpArray)\n",
    "#    for i in range(0, npArrayLen):\n",
    "#        hashList = []\n",
    "#        fhash1 = ppdeepNpArray[i]\n",
    "#        for j in range(0, npArrayLen):\n",
    "#            if i == j:\n",
    "#                continue\n",
    "#            fhash2 = ppdeepNpArray[j]\n",
    "#            hashList.append(ppdeep_diff(fhash1, fhash2))\n",
    "#        hashDiffList.append(calculate_arithmetic_mean(hashList))\n",
    "#    return hashDiffList\n",
    "\n",
    "#def calculate_mean_mse(imgNpArray):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    mseDiffList = []\n",
    "#    npArrayLen = len(imgNpArray)\n",
    "#    for i in range(0, npArrayLen):\n",
    "#        mseList = []\n",
    "#        img1 = imgNpArray[i]\n",
    "#        for j in range(0, npArrayLen):\n",
    "#            if i == j:\n",
    "#                continue\n",
    "#            img2 = imgNpArray[j]\n",
    "#            squared_diff = np.square(img1 - img2)\n",
    "#            mse = np.mean(squared_diff)\n",
    "#            mseList.append(mse)\n",
    "#        mseDiffList.append(calculate_rmse(mseList))\n",
    "#    return mseDiffList\n",
    "\n",
    "#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "#import networkx as nx\n",
    "#from node2vec import Node2Vec\n",
    "\n",
    "#def get_html_attributes(htmlDoc):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    def tag_depth(tag):\n",
    "#        depth = 0\n",
    "#        while tag.parent:\n",
    "#            tag = tag.parent\n",
    "#            depth += 1\n",
    "#        return depth\n",
    "#\n",
    "#    if not htmlDoc:\n",
    "#        return pd.Series([0, 0, 0, 0],\n",
    "#                         index=['http_document_tags', 'http_document_uniquetags', 'http_document_tagdepth', 'http_document_attributes'])\n",
    "#    soup = bs(htmlDoc, 'html.parser')\n",
    "#    tags = soup.find_all()\n",
    "#    htmlTags = len(tags)\n",
    "#    htmlUniqueTags = len(set(tag.name for tag in tags))\n",
    "#    depths = [tag_depth(tag) for tag in tags]\n",
    "#    htmlTagDepth = np.mean(depths)\n",
    "#    htmlAttributes = sum(len(tag.attrs) for tag in tags)\n",
    "#    return pd.Series([htmlTags, htmlUniqueTags, htmlTagDepth, htmlAttributes],\n",
    "#                     index=['http_document_tags', 'http_document_uniquetags', 'http_document_tagdepth', 'http_document_attributes'])\n",
    "\n",
    "#def get_image_entropy(imageData):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    image = image_load(imageData)\n",
    "#    return image_shannon_entropy(image)\n",
    "\n",
    "#def get_image_edges(imageData, grayscale_convert=True):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    image = image_load(imageData)\n",
    "#    if grayscale_convert:\n",
    "#        image = image.convert('L')\n",
    "#    npImage = np.array(image)\n",
    "#    edges = Canny(npImage, 100, 200)\n",
    "#    return np.sum(edges > 0)\n",
    "\n",
    "#def get_lbp_variance(imageData, grayscale_convert=True):\n",
    "#    \"\"\"\n",
    "#    Obsolete, if embeddings are used\n",
    "#    \"\"\"\n",
    "#    image = image_load(imageData)\n",
    "#    if grayscale_convert:\n",
    "#        image = image.convert('L')\n",
    "#    npImage = np.array(image)\n",
    "#    lbp = local_binary_pattern(npImage, P=8, R=1, method=\"uniform\")\n",
    "#    return np.var(lbp)\n",
    "\n",
    "### HTML document embeddings - doc2vec\n",
    "doc2vec = \"\"\"\n",
    "html_docs = frame['http.content'].to_list()\n",
    "tagged_docs = [TaggedDocument(words=bs(html, 'html.parser').get_text().split(), tags=[str(i)]) for i, html in enumerate(html_docs)]\n",
    "model = Doc2Vec(tagged_docs, vector_size=50, window=2, min_count=1, workers=4)\n",
    "html_embeddings = [model.dv[str(i)] for i in range(len(html_docs))]\n",
    "\"\"\"\n",
    "\n",
    "### HTML document embeddings - node2vec\n",
    "node2vec = \"\"\"\n",
    "def html_to_graph(htmlDoc):\n",
    "    soup = bs(htmlDoc, 'html.parser')\n",
    "    G = nx.DiGraph()\n",
    "    def add_edges(parent, soup):\n",
    "        for child in soup.children:\n",
    "            if child.name:\n",
    "                G.add_edge(parent, child.name)\n",
    "                add_edges(child.name, child)\n",
    "    for tag in soup.find_all(recursive=False):\n",
    "        G.add_node(tag.name)\n",
    "        add_edges(tag.name, tag)\n",
    "    return G\n",
    "\n",
    "graphs = [html_to_graph(htmlDoc) for htmlDoc in frame['http.content']]\n",
    "node2vec_models = []\n",
    "for G in graphs:\n",
    "    node2vec = Node2Vec(G, dimensions=32, walk_length=10, num_walks=100, workers=4)\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    node2vec_models.append(model)\n",
    "html_embeddings = [model.wv['html'] if 'html' in model.wv else np.zeros(64) for model in node2vec_models]\n",
    "\"\"\"\n",
    "\n",
    "# Feature extraction - extract various numeric features.\n",
    "# Replaced by embeddings\n",
    "\n",
    "#from cv2 import Canny\n",
    "#from skimage.measure import shannon_entropy as image_shannon_entropy\n",
    "#from skimage.feature import local_binary_pattern\n",
    "\n",
    "features = \"\"\"\n",
    "# HTTP content feature extraction\n",
    "frame[['http_document_tags', 'http_document_uniquetags', 'http_document_tagdepth', 'http_document_attributes']] = frame['http_document_content'].apply(get_html_attributes)\n",
    "frame['http_document_bytesize'] = [len(content.encode('utf-8')) for content in frame['http.content']]\n",
    "\n",
    "# HTTP image feature extraction\n",
    "frame['http_image_structentropy'] = frame['http.image'].apply(get_image_entropy)\n",
    "frame['http_image_dataentropy'] = frame['http.image'].apply(calculate_entropy)\n",
    "frame['http_image_edges'] = frame['http.image'].apply(get_image_edges, grayscale_convert=False)\n",
    "frame['http_image_lbp'] = frame['http.image'].apply(get_lbp_variance, grayscale_convert=False)\n",
    "\n",
    "contentFrame = frame[['http.entropy', 'http_document_bytesize', 'http_document_tags', 'http_document_uniquetags', 'http_document_tagdepth', 'http_document_attributes']]\n",
    "imageFrame = frame[['http_image_structentropy', 'http_image_dataentropy', 'http_image_edges', 'http_image_lbp']]\n",
    "\"\"\"\n",
    "\n",
    "def create_snapshot_diff_old(cc_client, snapshot_db=\"harvester\", snapshot_table=\"snapshot\", diff_table=\"cluster\", verbose=True):\n",
    "    \"\"\"\n",
    "    Obsolete, if embeddings are used.\n",
    "    TODO: Instead -- calculate embeddings and clusters, and save them to separate table.\n",
    "    \"\"\"\n",
    "    cc_client.command(f\"DROP TABLE IF EXISTS {snapshot_db}.{diff_table}\")\n",
    "    cc_client.command(f\"CREATE TABLE IF NOT EXISTS {snapshot_db}.{diff_table} (url_init String, meta_agentid String, diff_fuzzyhash Float64, diff_mse Float64) ENGINE = MergeTree() ORDER BY url_init\")\n",
    "    \n",
    "    urlList = cc_client.query(f\"SELECT DISTINCT url.init FROM {snapshot_db}.{snapshot_table} ORDER BY url.init\").result_columns[0]\n",
    "    u = 0\n",
    "    ucount = len(urlList)\n",
    "    for url in urlList:\n",
    "        startTime = time()\n",
    "        diffDf = client.query_df(f\"SELECT url.init, meta.agentid, http.fuzzyhash, http.image FROM {snapshot_db}.{snapshot_table} WHERE url.init=='{url}' ORDER BY meta.agentid ASC\")\n",
    "        # Process sha256 - can be queried and processed from the snapshot table directly\n",
    "        # diffDf['http.sha256'] = diffDf['http.sha256'].apply(convert_sha256_value)\n",
    "        # Process fuzzy-hash CTPH\n",
    "        diffDf['http.fuzzyhash'] = calculate_mean_ppdeep(diffDf['http.fuzzyhash'].to_numpy())\n",
    "        # Process PNG image MSE, SSIM\n",
    "        diffDf['http.image'] = diffDf['http.image'].apply(convert_image_array, resize=False)\n",
    "        diffDf['diff_mse'] = calculate_mean_mse(diffDf['http.image'].to_numpy())\n",
    "        diffDf = diffDf.drop('http.image', axis=1)\n",
    "\n",
    "        # Fixed in harvester syntax 0.52. While the snapshot file still uses dot synatx, retain this. Modify once the new snapshot format becomes the main one.\n",
    "        # Modify everywhere in the code!\n",
    "        diffDf.rename(columns={'url.init': 'url_init', 'meta.agentid': 'meta_agentid', 'http.fuzzyhash': 'diff_fuzzyhash'}, inplace=True)\n",
    "        \n",
    "        diffSchema = {\n",
    "            \"url_init\": \"string\",\n",
    "            \"meta_agentid\": \"string\",\n",
    "            \"diff_fuzzyhash\": \"float64\",\n",
    "            \"diff_mse\": \"float64\"\n",
    "            }\n",
    "        diffDf = diffDf.astype(diffSchema)\n",
    "        status = cc_client.insert_df(database=snapshot_db, table=diff_table, df=diffDf)\n",
    "        endTime = time()\n",
    "        u += 1\n",
    "        if verbose:\n",
    "            print(f\"{int(u / ucount * 100)}%  \\t ({u}/{ucount}) \\t {round((endTime - startTime), 2)}s \\t {url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
